{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import en_core_web_sm\n",
    "import en_coref_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature sentiment extraction module based on dependency graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency relation based feature sentiment extraction\n",
    "def feature_sentiment(sentence):\n",
    "    '''\n",
    "    function: dependency relation based feature sentiment extraction\n",
    "    input: dictionary and sentence\n",
    "    output: updated dictionary\n",
    "    '''\n",
    "    sent_dict = Counter()\n",
    "    sentence = spacy(sentence)\n",
    "    debug = 0\n",
    "#     print('token.text, token.pos_, token.dep_, token.head, token.head.dep_, token.children')\n",
    "    for token in sentence:\n",
    "#         print(token.text, token.pos_, token.dep_, token.head, token.head.dep_, [child for child in token.children])\n",
    "        #check if the word is an opinion word, then assign sentiment\n",
    "        if token.text in opinion_words:\n",
    "            senti_words = []            \n",
    "#             sentiment = 1 if token.text in pos else -1\n",
    "            if token.text in pos:\n",
    "                sentiment = 1\n",
    "            else:\n",
    "                sentiment = -1\n",
    "            senti_words = [token.text] + senti_words\n",
    "                \n",
    "            # if target is an adverb modifier (i.e. pretty, highly, etc.)\n",
    "            # but happens to be an opinion word, ignore and pass\n",
    "            if (token.dep_ == \"advmod\"):\n",
    "                continue\n",
    "            elif (token.dep_ == \"amod\"):\n",
    "                # check for negation\n",
    "                for child in token.head.head.children:\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if (child.dep_ == \"neg\"): \n",
    "                        sentiment *= -1\n",
    "                        senti_words = [child.text] + senti_words\n",
    "#                 sent_dict[token.head.text] += sentiment\n",
    "#                 sent_dict[(token.head.text, ' '.join(senti_words))] += sentiment\n",
    "                noun = token.head.text\n",
    "                # check for nouns\n",
    "                for child in token.head.children:\n",
    "                    if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n",
    "                        noun = child.text + \" \" + noun\n",
    "                        # Check for compound nouns\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.dep_ == \"compound\":\n",
    "                                noun = subchild.text + \" \" + noun\n",
    "                sent_dict[(noun, ' '.join(senti_words))] += sentiment\n",
    "\n",
    "            # for opinion words that are adjectives, adverbs, verbs...\n",
    "            else:\n",
    "                for child in token.children:\n",
    "                    # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\n",
    "                    # This could be better updated for modifiers that either positively or negatively emphasize\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        sentiment *= 1.5\n",
    "                        senti_words = [child.text] + senti_words\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if child.dep_ == \"neg\":\n",
    "                        sentiment *= -1\n",
    "                        senti_words = [child.text] + senti_words\n",
    "                for child in token.children:\n",
    "                    # if verb, check if there's a direct object\n",
    "                    if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \n",
    "#                         sent_dict[child.text] += sentiment\n",
    "                        sent_dict[(child.text, ' '.join(senti_words))] += sentiment\n",
    "                        # check for conjugates (a AND b), then add both to dictionary\n",
    "                        subchildren = []\n",
    "                        conj = 0\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.text == \"and\":\n",
    "                                conj=1\n",
    "                            if (conj == 1) and (subchild.text != \"and\"):\n",
    "                                subchildren.append(subchild.text)\n",
    "                                conj = 0\n",
    "                        for subchild in subchildren:\n",
    "#                             sent_dict[subchild] += sentiment\n",
    "                            sent_dict[(subchild, ' '.join(senti_words))] += sentiment\n",
    "\n",
    "                # check for negation\n",
    "                for child in token.head.children:\n",
    "                    noun = \"\"\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        sentiment *= 1.5\n",
    "                        senti_words = [child.text] + senti_words\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if (child.dep_ == \"neg\"): \n",
    "                        sentiment *= -1\n",
    "                        senti_words = [child.text] + senti_words\n",
    "                \n",
    "                # check for nouns\n",
    "                for child in token.head.children:\n",
    "                    noun = \"\"\n",
    "                    if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n",
    "                        noun = child.text\n",
    "                        # Check for compound nouns\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.dep_ == \"compound\":\n",
    "                                noun = subchild.text + \" \" + noun\n",
    "#                         sent_dict[noun] += sentiment\n",
    "                        sent_dict[(noun, ' '.join(senti_words))] += sentiment\n",
    "                    debug += 1\n",
    "    return sent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_sent(sentence):\n",
    "    '''\n",
    "    function: classify the sentence into a category, and assign sentiment\n",
    "    input: sentence & aspect dictionary, which is going to be updated\n",
    "    output: updated aspect dictionary\n",
    "    note: aspect_dict is a parent dictionary with all the aspects\n",
    "    '''\n",
    "    # get aspect names and their sentiment in a dictionary form\n",
    "    sent_dict = feature_sentiment(sentence)\n",
    "    return sent_dict\n",
    "\n",
    "def replace_pronouns(text):\n",
    "    '''\n",
    "    function: replaces the pronouns in a text with referring word\n",
    "    input: text that needs to be processed\n",
    "    output: pronoun replaced text\n",
    "    '''\n",
    "    doc = coref(text)\n",
    "    text_updated = ''\n",
    "    if doc._.coref_resolved == '':\n",
    "        text_updated = text\n",
    "    else:\n",
    "        text_updated = doc._.coref_resolved\n",
    "    return text_updated\n",
    "\n",
    "def split_sentence(text):\n",
    "    '''\n",
    "    function: splits review into a list of sentences using spacy's sentence parser\n",
    "    input: complete input text\n",
    "    output: list of sentences\n",
    "    '''\n",
    "    review = spacy(text)\n",
    "    bag_sentence = []\n",
    "    start = 0\n",
    "    for token in review:\n",
    "        if token.sent_start:\n",
    "            bag_sentence.append(review[start:(token.i-1)])\n",
    "            start = token.i\n",
    "        if token.i == len(review)-1:\n",
    "            bag_sentence.append(review[start:(token.i+1)])\n",
    "    return bag_sentence\n",
    "\n",
    "def remove_special_char(sentence):\n",
    "    '''\n",
    "    function: removes the special characters in a text\n",
    "    input: text that needs to be processed\n",
    "    output: special character removed text\n",
    "    '''\n",
    "    return re.sub(r\"[^a-zA-Z0-9.',:;?]+\", ' ', sentence)\n",
    "\n",
    "def review_pipe(review):\n",
    "    '''\n",
    "    function: processing pipeline performing step by step process for aspect sentiment \n",
    "    input: input review\n",
    "    output: aspect sentiment list \n",
    "    '''\n",
    "    review = replace_pronouns(review)\n",
    "    sentences = split_sentence(review)\n",
    "    review_dict = Counter()\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_char(str(sentence))\n",
    "        sent_dict = classify_and_sent(sentence.lower())\n",
    "        dict.update(review_dict,sent_dict)\n",
    "    return review_dict\n",
    "\n",
    "# feature sentiment mining for review\n",
    "def review_feature_sentiment_extraction(review):\n",
    "    '''\n",
    "    function: extracts feature sentiments for an input review\n",
    "    input: input review\n",
    "    output: feature sentiment list\n",
    "    '''\n",
    "    feat_senti_list = []\n",
    "    f_s_list = review_pipe(review).most_common()\n",
    "    for f_s in f_s_list:\n",
    "        f = f_s[0]\n",
    "        feat = f\n",
    "        # checking for exception features\n",
    "        feature = f[0]\n",
    "        opinion = f[1]\n",
    "        if feature in exception_features:\n",
    "            continue\n",
    "        s = f_s[1]\n",
    "        if s > 0:\n",
    "            senti = 'pos'\n",
    "        elif s < 0:\n",
    "            senti = 'neg'\n",
    "        else:\n",
    "            senti = 'neu'\n",
    "        feat_senti = feat, senti\n",
    "        feat_senti_list.append(feat_senti)\n",
    "    return feat_senti_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NLP resources\n",
    "spacy = en_core_web_sm.load()\n",
    "coref = en_coref_sm.load()\n",
    "\n",
    "# Load opinion lexicon\n",
    "neg_file = open(\"resources/opinion-lexicon-English/neg_words.txt\",encoding = \"ISO-8859-1\")\n",
    "pos_file = open(\"resources/opinion-lexicon-English/pos_words.txt\",encoding = \"ISO-8859-1\")\n",
    "neg = [line.strip() for line in neg_file.readlines()]\n",
    "pos = [line.strip() for line in pos_file.readlines()]\n",
    "opinion_words = neg + pos\n",
    "\n",
    "# Load exception feature lexicon\n",
    "exception_feature_file = open(\"resources/exception_features.txt\",encoding = \"ISO-8859-1\")\n",
    "exception_features = [line.strip() for line in exception_feature_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # test code for feature sentiment\n",
    "# review = \"I came here with my friends on a Tuesday night. The sushi here is amazing. Our waiter was very helpful, but the music was terrible.\"\n",
    "# # review = \"This is an awesome chicken sushi.\"\n",
    "# # review = \"This is not awesome chicken soup.\"\n",
    "# # review = \"The zipper is difficult to work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_sentiment_list = review_feature_sentiment_extraction(review)\n",
    "# feature_sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature sentiment extraction from Groupon deals and transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\env_cm\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (18,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54599, 24)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deal_clean = pd.read_csv('dataset/clean_deal_data.csv')\n",
    "df_deal_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86939, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review_clean = pd.read_csv('dataset/clean_review_data.csv')\n",
    "df_review_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86939, 31)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge review dataframe and deal dataframe\n",
    "df_transaction_clean = df_review_clean.merge(df_deal_clean, left_on=['dealid', 'deal_url'], right_on=['dealid', 'deal_url'])\n",
    "# df_transaction_clean.columns\n",
    "df_transaction_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dealid', 'deal_url', 'reviewid', 'review_rating', 'reviewer',\n",
       "       'purchaser_type', 'review_date', 'review_text', 'review_text_length',\n",
       "       'deal_title', 'bread_crumbs', 'crumb1', 'crumb2', 'crumb3', 'crumb4',\n",
       "       'crumb5', 'crumb6', 'crumb_last', 'crumbs_all', 'product_details',\n",
       "       'fine_print', 'breakout_messages', 'breakout_pricing', 'variations',\n",
       "       'merchant_info', 'merchant_location', 'gtmdata', 'merchant_profile',\n",
       "       'address', 'phone_number', 'dl_merchant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the groupon data\n",
    "df = df_transaction_clean[0:5]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dealid', 'reviewid', 'review_text'], dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df[['dealid','reviewid','review_text']]\n",
    "df_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Continuum\\miniconda3\\envs\\env_cm\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\programs\\Continuum\\miniconda3\\envs\\env_cm\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4370436668395996"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['review_text'] = df.review_text.astype(str)\n",
    "start_time = time.time()\n",
    "df_sample['feature_sentiment_extracted'] = df_sample['review_text'].apply(review_feature_sentiment_extraction)\n",
    "execution_time = (time.time() - start_time)\n",
    "execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('output/transaction_feature_sentiment_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
